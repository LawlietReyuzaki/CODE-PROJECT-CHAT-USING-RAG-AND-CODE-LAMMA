{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ac7683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 21:07:35.234 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /usr/lib/python3/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'embeddings' already exists.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "\n",
    "#from PyPDF2 import PdfReader\n",
    "import os\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "from utils import get_embeddings\n",
    "\n",
    "\n",
    "#for chatting with the code\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "import pickle as pkl\n",
    "import gradio as gr\n",
    "from utils import get_embeddings\n",
    "from llm import LLM\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "path = 'embeddings'\n",
    "chunk_size = 3\n",
    "overlap = 0\n",
    "split_char = '.'\n",
    "\n",
    "folder_name = 'embeddings'\n",
    "name = 'code_embeddings' #file name\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\n",
    "embeddings_model = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\n",
    "embeddings_model.to(device)\n",
    "embeddings_model.eval()\n",
    "\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(f\"Folder '{folder_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists.\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "def get_answer(query, k):\n",
    "    query = query\n",
    "    query_embedding = get_embeddings([query], tokenizer, embeddings_model)\n",
    "    scores, text_idx = index.search(query_embedding, k)\n",
    "    text_idx = text_idx.flatten()\n",
    "    info = '\\n--------\\n'.join(np.array(text_info)[text_idx])\n",
    "    info = info.replace('passage: ', '').strip()\n",
    "    ans = chatbot.get_response(query, info)\n",
    "    return ans + '\\n-------------Information: \\n' + info + '\\n---------\\n'\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Chuncker:\n",
    "    def __init__(self, chunck_size=3, chunk_overlap=1, split_char='.'):\n",
    "        self.chunck_size = chunck_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.split_char = split_char\n",
    "\n",
    "    def get_chunks(self, text):\n",
    "        all_words = text.split(self.split_char)\n",
    "        chunks = []\n",
    "        for i in range(0, len(all_words), self.chunck_size - self.chunk_overlap):\n",
    "            chunks.append('passage: ' + ' '.join(all_words[i:i + self.chunck_size]))\n",
    "        return chunks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set Streamlit theme to dark\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "    <style>\n",
    "    .reportview-container {\n",
    "        background: #1E1E1E;\n",
    "        color: #FFFFFF;\n",
    "    }\n",
    "    .stTextInput>div>div>input {\n",
    "        color: #000000;\n",
    "    }\n",
    "    .st-bm {\n",
    "        background-color: #2E2E2E !important;\n",
    "        color: #FFFFFF !important;\n",
    "    }\n",
    "    .st-bq {\n",
    "        border-color: #FFFFFF;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True\n",
    ")\n",
    "command = 'ollama run codellama'\n",
    "\n",
    "def aggregate_js_files(folder_path):\n",
    "    if not os.path.exists('aggregated_text'):\n",
    "        os.makedirs('aggregated_text')\n",
    "    \n",
    "    with open(os.path.join('aggregated_text', 'aggregated_code.txt'), 'w') as aggregated_file:\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if file.endswith(('.js','.ts')):\n",
    "                    aggregated_file.write(f\"\\n\\n// File: {file}\\n\\n\")\n",
    "                    with open(os.path.join(root, file), 'r') as js_file:\n",
    "                        js_content = js_file.read()\n",
    "                        aggregated_file.write(js_content)\n",
    "\n",
    "def execute_command_with_input(input_string):\n",
    "    try:\n",
    "        proc = subprocess.Popen(command, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        proc.stdin.write(input_string + '\\n')\n",
    "        proc.stdin.flush()\n",
    "        output, error = proc.communicate()\n",
    "        if proc.returncode != 0:\n",
    "            return f\"Error: {error}\"\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def main():\n",
    "    \n",
    "    global command\n",
    "    extract_dir = 'extracted'\n",
    "    st.title(\"Code Summary Generator\")\n",
    "\n",
    "    uploaded_zip = st.file_uploader(\"Upload Zip File Containing Angular Code\", type=\"zip\")\n",
    "    if uploaded_zip is not None:\n",
    "        with open(uploaded_zip.name, \"wb\") as f:\n",
    "            f.write(uploaded_zip.getvalue())\n",
    "\n",
    "        # Create a directory to extract the contents of the zip file\n",
    "        extract_dir = os.path.join(os.getcwd(), \"extracted\")\n",
    "        if not os.path.exists(extract_dir):\n",
    "            os.makedirs(extract_dir)\n",
    "\n",
    "        # Extract the zip file into the \"extracted\" directory\n",
    "        with zipfile.ZipFile(uploaded_zip.name, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_dir)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\n",
    "    embeddings_model = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\n",
    "    extracted_folder_path = extract_dir if os.path.exists(extract_dir) else None\n",
    "\n",
    "    if st.button(\"Pass to Model\") and extracted_folder_path:\n",
    "        aggregate_js_files(extracted_folder_path)\n",
    "\n",
    "        with open(os.path.join('aggregated_text', 'aggregated_code.txt'), 'r') as aggregated_file:\n",
    "            file_contents = aggregated_file.read()\n",
    "\n",
    "        st.subheader(\"Uploaded Project Contents are as follows: \")\n",
    "        st.text_area(\"\", value=file_contents, height=200, key=\"uploaded_file_contents\")\n",
    "        \n",
    "        chunker = Chuncker(chunck_size=chunk_size, chunk_overlap=overlap, split_char=split_char)\n",
    "\n",
    "        index = faiss.IndexFlatL2(embeddings_model.config.hidden_size)\n",
    "        #make index of the text file\n",
    "        text_info = []\n",
    "\n",
    "\n",
    "        text = open('aggregated_text/aggregated_code.txt', 'r').read()\n",
    "\n",
    "        text_chunks = chunker.get_chunks(text)\n",
    "        text_info.extend([x for x in text_chunks])\n",
    "        \n",
    "        \n",
    "        # The following is probably slow, should batch this\n",
    "    \n",
    "\n",
    "        if not (os.path.exists(index_file_path) or os.path.exists(text_info_file_path)):\n",
    "\n",
    "            text_embeddings = np.array([get_embeddings([x], tokenizer, embeddings_model) for x in tqdm(text_chunks)]).reshape(len(text_chunks), -1)\n",
    "            index.add(text_embeddings)\n",
    "            name = \"code_embeddings\"\n",
    "\n",
    "            index_file_path = f'embeddings/{name}.index'\n",
    "            text_info_file_path = f'embeddings/{name}_text_info.pkl'\n",
    "\n",
    "            faiss.write_index(index, f'embeddings/{name}.index')\n",
    "\n",
    "            pkl.dump(text_info, open(f'embeddings/{name}_text_info.pkl', 'wb'))\n",
    "\n",
    "        \n",
    "        \n",
    "        #===================================================================================\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # device = 'cpu'\n",
    "        embeddings_model.to(device)\n",
    "        embeddings_model.eval()\n",
    "        pass\n",
    "\n",
    "        chatbot = LLM()\n",
    "        index = faiss.read_index(f'embeddings/{name}.index')\n",
    "        text_info = pkl.load(open(f'embeddings/{name}_text_info.pkl', 'rb'))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        #+----------------------------------------------------------------------------------\n",
    "        #CODE FOR GENERTATING GRAPH USING OLLAMA        \n",
    "        \n",
    "#         st.subheader(\"Response\")\n",
    "#         st.text_area(\"\", value=response, height=200, key=\"response_text_area\")\n",
    "\n",
    "#         file_contents += 'generate graph data output, Json data output for this angular code'\n",
    "\n",
    "#         response = get_answer(file_contents,7)\n",
    "        \n",
    "#         try:\n",
    "#             #graph_data = json.loads(response)\n",
    "#             with open(\"graph_data.txt\", \"w\") as json_file:\n",
    "#                 json.dump(graph_data, json_file)\n",
    "#         except Exception as e:\n",
    "#             st.error(f\"Error saving graph data: {e}\")\n",
    "\n",
    "        #+----------------------------------------------------------------------------------\n",
    "        #CODE FOR CHATING WITH PROJECT USING RAG       \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "    st.subheader(\"Query Chat\")\n",
    "    user_query = st.text_input(\"Ask a question:\")\n",
    "    if user_query:\n",
    "        response = get_answer(user_query,7)\n",
    "        st.text_area(\"\", value=response, height=100, key=\"query_response_text_area\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e94ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"graph_data.txt\", \"w\") as File:\n",
    "    File.write('response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced8fb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
